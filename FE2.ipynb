{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5729374a-8729-4eb6-9158-5d16a1d3951c",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655ec178-3eec-4178-8df4-8d870166aeda",
   "metadata": {},
   "source": [
    "**Min-Max scaling** is a data preprocessing technique used to transform numeric features in a dataset to a specific range, typically between 0 and 1. It linearly scales the values of each feature, preserving the relative relationships between data points while ensuring that all values fall within the specified range.\n",
    "\n",
    "The formula for Min-Max scaling is:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca116699-780d-4da4-9174-e56e68f60422",
   "metadata": {},
   "source": [
    "**Xsc = X - Xmin / Xmax - Xmin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af519fb3-b1f7-43a4-ab74-7f1ef3ac5615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MinMaxScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MinMaxScaler()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "scaler = MinMaxScaler()\n",
    "print(scaler.fit(data))\n",
    "MinMaxScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96fc531a-e4e1-4d12-9047-0947c8af7d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. 18.]\n"
     ]
    }
   ],
   "source": [
    " print(scaler.data_max_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81cd780-22c1-4099-8dc2-4968b0ebaa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.  ]\n",
      " [0.25 0.25]\n",
      " [0.5  0.5 ]\n",
      " [1.   1.  ]]\n"
     ]
    }
   ],
   "source": [
    "print(scaler.transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76794c78-703f-440c-960b-9d6de60e25b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.5 0. ]]\n"
     ]
    }
   ],
   "source": [
    "print(scaler.transform([[2, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4124307-6f88-4ab8-a5f2-bfda0dc378e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (Scaled): [[0.        ]\n",
      " [0.23529412]\n",
      " [0.41176471]\n",
      " [0.70588235]\n",
      " [1.        ]]\n",
      "Bedrooms (Scaled): [[0.        ]\n",
      " [0.33333333]\n",
      " [0.66666667]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "size = np.array([800, 1200, 1500, 2000, 2500]).reshape(-1, 1)\n",
    "bedrooms = np.array([2, 3, 4, 5]).reshape(-1, 1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "size_scaled = scaler.fit_transform(size)\n",
    "bedrooms_scaled = scaler.fit_transform(bedrooms)\n",
    "\n",
    "print(\"Size (Scaled):\", size_scaled)\n",
    "print(\"Bedrooms (Scaled):\", bedrooms_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ed047-9862-4d45-8a52-a806c03fbe5e",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850f65b7-135b-4469-9e86-cd2ef3e51fc8",
   "metadata": {},
   "source": [
    " **Unit Vector scaling**, also known as **Normalization**, is a feature scaling technique used to transform numeric features in a dataset so that they have a unit magnitude or length. In other words, it scales the features to have a Euclidean norm (magnitude) of 1. This technique is often used when the direction or angle between data points is more important than their absolute values.\n",
    "\n",
    "The formula for Unit Vector scaling of a feature vector **X** is as follows:\n",
    "- **X(normalized)=X/|X|**\n",
    "##### Here's how Unit Vector scaling differs from Min-Max scaling:\n",
    "\n",
    "1. **Scale Range:**\n",
    "   - Min-Max scaling scales the features to a specific range (e.g., between 0 and 1), ensuring that all values fall within this range.\n",
    "   - Unit Vector scaling scales the features to have a magnitude of 1, which means they lie on the unit circle in multi-dimensional space. It doesn't constrain them to a specific numerical range.\n",
    "\n",
    "2. **Magnitude vs. Direction:**\n",
    "   - Min-Max scaling preserves the magnitude and direction of the original features but scales them proportionally to fit within the specified range.\n",
    "   - Unit Vector scaling preserves the direction of the features but normalizes their magnitude to 1. It's particularly useful when the direction of the data vectors matters more than their absolute values.\n",
    "\n",
    "**Example: Unit Vector Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f872af-e70e-4d54-bddd-0090042c4f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[ 3  0 -4  6]\n",
      " [ 0  2 -3  4]]\n",
      "\n",
      "Normalized Data (Unit Vectors):\n",
      "[[ 0.38411064  0.         -0.51214752  0.76822128]\n",
      " [ 0.          0.37139068 -0.55708601  0.74278135]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Original dataset (features as rows and columns as features)\n",
    "data = np.array([[3, 0, -4, 6],\n",
    "                 [0, 2, -3, 4]])\n",
    "\n",
    "# Normalize the dataset to unit vectors using scikit-learn's normalize function\n",
    "normalized_data = normalize(data, norm='l2', axis=1)\n",
    "\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nNormalized Data (Unit Vectors):\")\n",
    "print(normalized_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8597c-fce5-4914-965d-4107cc68e0da",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an  example to illustrate its application. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950cac47-c814-430d-a054-2aa21c0dd472",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique widely used in data analysis and machine learning. It is used to transform high-dimensional data into a lower-dimensional form while preserving as much of the variance or information in the data as possible. PCA accomplishes this by identifying and capturing the underlying structure or patterns in the data.\n",
    "\n",
    "Here's a step-by-step overview of how PCA works:\n",
    "\n",
    "1. **Data Centering:** PCA begins by centering the data, which means subtracting the mean (average) value of each feature from the data points. Centering helps remove any bias in the data.\n",
    "\n",
    "2. **Covariance Matrix:** PCA computes the covariance matrix of the centered data. The covariance matrix describes how different features in the data vary together and provides insights into their relationships and dependencies.\n",
    "\n",
    "3. **Eigenvalue Decomposition:** PCA performs an eigenvalue decomposition (or singular value decomposition) of the covariance matrix. This decomposition yields eigenvalues and eigenvectors.\n",
    "\n",
    "4. **Principal Components:** The eigenvectors represent the directions (axes) in the original feature space along which the data varies the most. These eigenvectors are called principal components. The corresponding eigenvalues indicate the variance of the data along those directions.\n",
    "\n",
    "5. **Dimensionality Reduction:** PCA sorts the eigenvalues in descending order and selects the top k eigenvectors (principal components) that capture the most variance in the data. By choosing a smaller number of principal components (k), PCA effectively reduces the dimensionality of the dataset from the original number of features to k features.\n",
    "\n",
    "6. **Projection:** The original data is projected onto the new lower-dimensional space defined by the selected principal components. This projection results in a new dataset with reduced dimensions.\n",
    "\n",
    "PCA is commonly used for various purposes, including:\n",
    "\n",
    "- **Dimensionality Reduction:** Reducing the dimensionality of high-dimensional datasets, which can lead to more efficient computation and visualization.\n",
    "\n",
    "- **Noise Reduction:** Eliminating noise or irrelevant features, which can improve the performance of machine learning algorithms.\n",
    "\n",
    "- **Data Compression:** Storing or transmitting data more efficiently by representing it in a lower-dimensional form.\n",
    "\n",
    "- **Visualization:** Reducing the dimensionality of data for visualization purposes, allowing for easier exploration and understanding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c584631-3070-4306-a6fa-afe91438ea17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "\n",
      "Transformed Data (2 Principal Components):\n",
      "[[ 7.79422863e+00  4.41704682e-16]\n",
      " [ 2.59807621e+00 -1.20464913e-16]\n",
      " [-2.59807621e+00  1.20464913e-16]\n",
      " [-7.79422863e+00  3.61394740e-16]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample data (3 features)\n",
    "data = np.array([[1, 2, 3],\n",
    "                 [4, 5, 6],\n",
    "                 [7, 8, 9],\n",
    "                 [10, 11, 12]])\n",
    "\n",
    "# Create a PCA instance with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data to the first 2 principal components\n",
    "transformed_data = pca.fit_transform(data)\n",
    "\n",
    "# Print the transformed data\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nTransformed Data (2 Principal Components):\")\n",
    "print(transformed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8ca35-552b-4c23-8e89-a74f67931b17",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature  Extraction? Provide an example to illustrate this concept. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15db4dcc-5f31-4bc4-a1a4-386fa8088645",
   "metadata": {},
   "source": [
    "**Principal Component Analysis (PCA)** and **Feature Extraction** are closely related concepts in the context of dimensionality reduction and data analysis. PCA can be used as a feature extraction technique, allowing you to transform a dataset into a set of new features (principal components) that capture the most important information while reducing dimensionality.\n",
    "\n",
    "Here's the relationship between PCA and Feature Extraction and an example to illustrate it:\n",
    "\n",
    "**Relationship between PCA and Feature Extraction:**\n",
    "\n",
    "- **PCA as Feature Extraction:** PCA is a dimensionality reduction technique, but it can also be used as a feature extraction method. Instead of using all original features, you can use PCA to extract a smaller set of new features (principal components) that are linear combinations of the original features.\n",
    "\n",
    "- **Preservation of Information:** PCA selects the principal components in such a way that they capture the maximum variance in the data. These principal components retain the most important information while discarding less important information. In this sense, PCA performs feature extraction by creating a compact representation of the data.\n",
    "\n",
    "- **Dimensionality Reduction:** The extracted principal components are typically fewer in number than the original features, which results in dimensionality reduction. This can lead to more efficient computation and potentially improve the performance of machine learning algorithms.\n",
    "\n",
    "**Example: Using PCA for Feature Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "329c93ac-23bd-4230-a065-41221ef14f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data (3D):\n",
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "\n",
      "Extracted Features (2D):\n",
      "[[ 7.79422863e+00  4.41704682e-16]\n",
      " [ 2.59807621e+00 -1.20464913e-16]\n",
      " [-2.59807621e+00  1.20464913e-16]\n",
      " [-7.79422863e+00  3.61394740e-16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample data with three features (3D data)\n",
    "data = np.array([[1, 2, 3],\n",
    "                 [4, 5, 6],\n",
    "                 [7, 8, 9],\n",
    "                 [10, 11, 12]])\n",
    "\n",
    "# Create a PCA instance with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data to extract two principal components (features)\n",
    "extracted_features = pca.fit_transform(data)\n",
    "\n",
    "# Print the original data and the extracted features\n",
    "print(\"Original Data (3D):\")\n",
    "print(data)\n",
    "print(\"\\nExtracted Features (2D):\")\n",
    "print(extracted_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b08a1c-47ed-4a82-b6f8-137e21e2264c",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset  contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to  preprocess the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404912b-c047-4bcc-8b96-da330748380b",
   "metadata": {},
   "source": [
    "- Understand the Data\n",
    "- Apply Min-Max Scaling\n",
    "- Normalization Range [0, 1]\n",
    "- Repeat for Each Feature\n",
    "- Use the Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b80cd0a-8684-43b4-af4d-3b01e23a501a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated DataFrames:\n",
      "   Price  Rating  Delivery Time  Scaled Price  Scaled Rating  \\\n",
      "0   10.0     4.5           30.0      0.000000       0.636364   \n",
      "1   20.0     3.8           45.0      0.666667       0.000000   \n",
      "2   15.0     4.0           40.0      0.333333       0.181818   \n",
      "3   25.0     4.9           35.0      1.000000       1.000000   \n",
      "\n",
      "   Scaled Delivery Time  \n",
      "0              0.000000  \n",
      "1              1.000000  \n",
      "2              0.666667  \n",
      "3              0.333333  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample dataset with features: price, rating, and delivery time\n",
    "data = np.array([[10.0, 4.5, 30],\n",
    "                 [20.0, 3.8, 45],\n",
    "                 [15.0, 4.0, 40],\n",
    "                 [25.0, 4.9, 35]])\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data using Min-Max scaling\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Convert the original data to a DataFrame\n",
    "original_df = pd.DataFrame(data, columns=['Price', 'Rating', 'Delivery Time'])\n",
    "\n",
    "# Convert the scaled data to a DataFrame\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=['Scaled Price', 'Scaled Rating', 'Scaled Delivery Time'])\n",
    "\n",
    "concatenated_df = pd.concat([original_df, scaled_df], axis=1)\n",
    "\n",
    "# Print the concatenated DataFrame\n",
    "print(\"Concatenated DataFrames:\")\n",
    "print(concatenated_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5ff2a6-2b31-4cf8-8937-85da5f334580",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many  features, such as company financial data and market trends. Explain how you would use PCA to reduce the  dimensionality of the dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c9cf8-d84d-4eb0-9917-1faf615fc392",
   "metadata": {},
   "source": [
    "- Data Preprocessing\n",
    "- Feature Selection and Engineering\n",
    "- Standardization or Normalization\n",
    "- Applying PCA\n",
    "- Selecting Principal Components\n",
    "- Reduced-Dimension Dataset\n",
    "- Model Building\n",
    "- Model Evaluation and Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9257783f-7ea1-49b4-aa91-e4d27ef2ceed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Series</th>\n",
       "      <th>Prev Close</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Last</th>\n",
       "      <th>Close</th>\n",
       "      <th>VWAP</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Turnover</th>\n",
       "      <th>Trades</th>\n",
       "      <th>Deliverable Volume</th>\n",
       "      <th>%Deliverble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-11-27</td>\n",
       "      <td>MUNDRAPORT</td>\n",
       "      <td>EQ</td>\n",
       "      <td>440.00</td>\n",
       "      <td>770.00</td>\n",
       "      <td>1050.00</td>\n",
       "      <td>770.0</td>\n",
       "      <td>959.0</td>\n",
       "      <td>962.90</td>\n",
       "      <td>984.72</td>\n",
       "      <td>27294366</td>\n",
       "      <td>2.687719e+15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9859619</td>\n",
       "      <td>0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-11-28</td>\n",
       "      <td>MUNDRAPORT</td>\n",
       "      <td>EQ</td>\n",
       "      <td>962.90</td>\n",
       "      <td>984.00</td>\n",
       "      <td>990.00</td>\n",
       "      <td>874.0</td>\n",
       "      <td>885.0</td>\n",
       "      <td>893.90</td>\n",
       "      <td>941.38</td>\n",
       "      <td>4581338</td>\n",
       "      <td>4.312765e+14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1453278</td>\n",
       "      <td>0.3172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-11-29</td>\n",
       "      <td>MUNDRAPORT</td>\n",
       "      <td>EQ</td>\n",
       "      <td>893.90</td>\n",
       "      <td>909.00</td>\n",
       "      <td>914.75</td>\n",
       "      <td>841.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>884.20</td>\n",
       "      <td>888.09</td>\n",
       "      <td>5124121</td>\n",
       "      <td>4.550658e+14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1069678</td>\n",
       "      <td>0.2088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-11-30</td>\n",
       "      <td>MUNDRAPORT</td>\n",
       "      <td>EQ</td>\n",
       "      <td>884.20</td>\n",
       "      <td>890.00</td>\n",
       "      <td>958.00</td>\n",
       "      <td>890.0</td>\n",
       "      <td>929.0</td>\n",
       "      <td>921.55</td>\n",
       "      <td>929.17</td>\n",
       "      <td>4609762</td>\n",
       "      <td>4.283257e+14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1260913</td>\n",
       "      <td>0.2735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-12-03</td>\n",
       "      <td>MUNDRAPORT</td>\n",
       "      <td>EQ</td>\n",
       "      <td>921.55</td>\n",
       "      <td>939.75</td>\n",
       "      <td>995.00</td>\n",
       "      <td>922.0</td>\n",
       "      <td>980.0</td>\n",
       "      <td>969.30</td>\n",
       "      <td>965.65</td>\n",
       "      <td>2977470</td>\n",
       "      <td>2.875200e+14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>816123</td>\n",
       "      <td>0.2741</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Symbol Series  Prev Close    Open     High    Low   Last  \\\n",
       "0  2007-11-27  MUNDRAPORT     EQ      440.00  770.00  1050.00  770.0  959.0   \n",
       "1  2007-11-28  MUNDRAPORT     EQ      962.90  984.00   990.00  874.0  885.0   \n",
       "2  2007-11-29  MUNDRAPORT     EQ      893.90  909.00   914.75  841.0  887.0   \n",
       "3  2007-11-30  MUNDRAPORT     EQ      884.20  890.00   958.00  890.0  929.0   \n",
       "4  2007-12-03  MUNDRAPORT     EQ      921.55  939.75   995.00  922.0  980.0   \n",
       "\n",
       "    Close    VWAP    Volume      Turnover  Trades  Deliverable Volume  \\\n",
       "0  962.90  984.72  27294366  2.687719e+15     NaN             9859619   \n",
       "1  893.90  941.38   4581338  4.312765e+14     NaN             1453278   \n",
       "2  884.20  888.09   5124121  4.550658e+14     NaN             1069678   \n",
       "3  921.55  929.17   4609762  4.283257e+14     NaN             1260913   \n",
       "4  969.30  965.65   2977470  2.875200e+14     NaN              816123   \n",
       "\n",
       "   %Deliverble  \n",
       "0       0.3612  \n",
       "1       0.3172  \n",
       "2       0.2088  \n",
       "3       0.2735  \n",
       "4       0.2741  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv('ADANIPORTS.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4d1ce6b-b873-48fd-8beb-854922619f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the target variable from the features\n",
    "X = df.drop(columns=['Turnover','Trades','Symbol','Date','Series'])\n",
    "y = df['Turnover']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11857312-db27-4c1d-9cca-7d748c4dd4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 7.348374519676787e+27\n"
     ]
    }
   ],
   "source": [
    "# Standardize the features (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=10)  # Choose the number of components you want to retain\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a regression model (e.g., Linear Regression) on the reduced-dimension data\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af27c37-d636-4126-8d79-72afedd74859",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the  values to a range of 0 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8318985-674f-4fe5-a02e-4c6613a6ac2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.        ]\n",
      " [0.21052632]\n",
      " [0.47368421]\n",
      " [0.73684211]\n",
      " [1.        ]]\n"
     ]
    }
   ],
   "source": [
    "data= np.array([1, 5, 10, 15, 20]).reshape(-1, 1)\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data using Min-Max scaling\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185ef615-fd24-4228-b360-61815a84ddea",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform  Feature Extraction using PCA. How many principal components would you choose to retain, and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b4c0d4-9b80-40a5-b345-9468266fd68f",
   "metadata": {},
   "source": [
    "The number of principal components (PCs) to retain in a PCA-based feature extraction process depends on several factors, including the specific goals of your analysis and the amount of variance you want to preserve. To determine how many principal components to retain, you can consider the cumulative explained variance and the trade-off between dimensionality reduction and information loss.\n",
    "\n",
    "Here's a general process to decide how many principal components to retain for feature extraction:\n",
    "\n",
    "1. **Compute Explained Variance:** After applying PCA to your dataset, you'll have access to the explained variance for each principal component. Explained variance indicates how much of the original variance in the data is captured by each PC.\n",
    "\n",
    "2. **Plot Explained Variance:** Plot the cumulative explained variance as a function of the number of principal components. This plot is often referred to as a \"scree plot\" or \"explained variance plot.\"\n",
    "\n",
    "3. **Select a Threshold:** Decide on a threshold for the amount of variance you want to preserve. This threshold could be a specific percentage of the total variance (e.g., 95% or 99%) or a specific number of components (e.g., retain the top k components).\n",
    "\n",
    "4. **Determine the Number of Components:** Choose the number of principal components that allows you to meet or exceed your chosen threshold. This number is the one you'll retain for feature extraction.\n",
    "\n",
    "5. **Consider Practicality:** Consider practical considerations, such as the computational complexity of your analysis and the specific requirements of your downstream tasks. Sometimes, a balance between dimensionality reduction and information loss is necessary.\n",
    "\n",
    "6. **Validate Your Choice:** You can also perform cross-validation or other model evaluation techniques to assess the impact of dimensionality reduction on the performance of your machine learning models. This can help you confirm that the chosen number of components is appropriate for your specific task.\n",
    "\n",
    "The choice of how many principal components to retain would depend on your objectives. Here are a few considerations:\n",
    "\n",
    "- If you want to reduce dimensionality significantly and are primarily interested in capturing the most prominent patterns in the data, you might choose to retain a relatively small number of components (e.g., 2 or 3).\n",
    "  \n",
    "- If you want to balance dimensionality reduction with preserving a substantial amount of information, you might aim for a higher percentage of explained variance (e.g., 95% or more).\n",
    "\n",
    "- The inclusion of the \"gender\" feature, which is categorical, may impact your choice. You might consider encoding it as binary values (e.g., 0 for male and 1 for female) before applying PCA.\n",
    "\n",
    "Ultimately, the number of principal components to retain should align with your specific goals and the characteristics of your dataset. It may require experimentation and iterative analysis to find the most suitable balance between dimensionality reduction and information retention for your particular use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ebfb56b-e69e-47da-8e0c-c3356fb521a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio:\n",
      "[0.95388929 0.99576736 0.99923813 1.        ]\n",
      "\n",
      "Number of Components to Retain (95% of Variance): 1\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'height': [160, 170, 155, 175, 180],\n",
    "    'weight': [60, 70, 65, 80, 85],\n",
    "    'age': [30, 40, 35, 45, 50],\n",
    "    'gender': [0, 1, 0, 1, 1],  # Example encoding: 0 for male, 1 for female\n",
    "    'blood_pressure': [120, 130, 110, 140, 150]\n",
    "}\n",
    "\n",
    "# Convert the dataset to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Separate the features (X) and target (if applicable)\n",
    "X = df.drop(columns=['gender'])  # Exclude the 'gender' column for PCA\n",
    "\n",
    "# Standardize the features (mean=0, variance=1)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Calculate the cumulative explained variance\n",
    "explained_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Determine the number of components to retain (e.g., retain 95% of variance)\n",
    "n_components_to_retain = np.argmax(explained_variance_ratio >= 0.95) + 1\n",
    "\n",
    "# Print the explained variance and the chosen number of components\n",
    "print(\"Explained Variance Ratio:\")\n",
    "print(explained_variance_ratio)\n",
    "print(\"\\nNumber of Components to Retain (95% of Variance):\", n_components_to_retain)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
