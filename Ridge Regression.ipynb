{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69cdabe0-a354-41b2-ae7e-0b71a8433ce2",
   "metadata": {},
   "source": [
    "## Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "attachments": {
    "d33bb2d6-e741-4fc3-ad49-cf8663808865.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "4cf824c0-197b-4302-a8f3-5deb2de7a169",
   "metadata": {},
   "source": [
    "Ridge Regression, also known as Tikhonov regularization or L2 regularization, is a linear regression technique that is used to overcome the problems of multicollinearity in ordinary least squares (OLS) regression. In OLS, when there is multicollinearity (high correlation among predictor variables), the estimates of the regression coefficients become highly sensitive to random errors in the observed data. Ridge Regression introduces a regularization term, often denoted by λ (lambda), which penalizes large coefficients and helps to stabilize and improve the performance of the model.\n",
    "\n",
    "The Ridge Regression objective function is:\n",
    "\n",
    "![eq5-1-1024x20690581.png](attachment:d33bb2d6-e741-4fc3-ad49-cf8663808865.png)\n",
    "\n",
    "Here, RSS is the residual sum of squares, and the second term is the regularization term, where λ is the tuning parameter that controls the strength of the regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e72e27-ab60-481b-aaf7-116c1d827bc3",
   "metadata": {},
   "source": [
    "## Q2. What are the assumptions of Ridge Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585740a9-3bd7-49a7-a8c5-9171199f2b5d",
   "metadata": {},
   "source": [
    "*Ridge Regression, like ordinary least squares (OLS) regression, relies on certain assumptions to be valid. The main assumptions of Ridge Regression are similar to those of linear regression. Here are the key assumptions:*\n",
    "\n",
    "- Linearity: Ridge Regression assumes a linear relationship between the independent variables and the dependent variable. The model is formulated as a linear combination of the predictors with coefficients that need to be estimated.\n",
    "\n",
    "- Independence of Errors: The errors (residuals) in Ridge Regression should be independent of each other. In other words, the value of the error term for one observation should not provide information about the error term for another observation.\n",
    "\n",
    "- Homoscedasticity: The variance of the errors should be constant across all levels of the independent variables. This means that the spread of residuals should be consistent throughout the range of predicted values.\n",
    "\n",
    "- Normality of Errors: While Ridge Regression is not as sensitive to this assumption as ordinary least squares, it still assumes that the errors are normally distributed. This assumption is more critical for making statistical inferences and constructing confidence intervals.\n",
    "\n",
    "- No Perfect Multicollinearity: Ridge Regression assumes that there is no perfect multicollinearity among the independent variables. Perfect multicollinearity occurs when one or more independent variables are perfectly correlated, making it impossible to estimate the unique contribution of each variable.\n",
    "\n",
    "It's important to note that Ridge Regression is particularly useful when there are issues with multicollinearity in the dataset. The regularization term added to the objective function helps stabilize the estimation of coefficients in the presence of highly correlated predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c3023-c6b4-4b13-8680-50188e65e54b",
   "metadata": {},
   "source": [
    "## Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb57f75-de03-4921-baa5-9167273d4b68",
   "metadata": {},
   "source": [
    "#### Selection of Tuning Parameter (λ):\n",
    "The tuning parameter in Ridge Regression is typically denoted as λ (lambda), and it controls the strength of the regularization. The process of selecting the appropriate value for λ involves finding a balance between fitting the model well to the training data and preventing overfitting. Common approaches for selecting λ include:\n",
    "\n",
    "- Cross-Validation:\n",
    "\n",
    "One of the most common methods for tuning λ is cross-validation. The dataset is split into multiple subsets, and the model is trained on different combinations of training and validation sets. The performance of the model is then evaluated on the validation sets for each combination of λ.\n",
    "Common types of cross-validation include k-fold cross-validation and leave-one-out cross-validation. The value of λ that results in the best performance (e.g., lowest mean squared error) on the validation sets is chosen.\n",
    "\n",
    "- Grid Search:\n",
    "\n",
    "A grid search involves specifying a range of possible λ values and systematically trying each one to find the optimal value. This can be computationally intensive but is effective, especially if the range of potential λ values is relatively small.\n",
    "The grid search can be combined with cross-validation to evaluate the performance of the model for each λ value.\n",
    "\n",
    "- Regularization Paths:\n",
    "\n",
    "Some algorithms, such as coordinate descent or gradient descent-based methods, allow you to compute the regularization path, which shows how the coefficients change with varying λ.\n",
    "By observing how the coefficients evolve, you can identify the region of λ that provides a good balance between fitting the data and regularization.\n",
    "\n",
    "- Information Criteria:\n",
    "\n",
    "Information criteria, such as Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC), can be used to select the optimal λ. These criteria balance the goodness of fit and model complexity.\n",
    "Lower values of AIC or BIC indicate better-fitting models, so you can choose the λ that minimizes these criteria.\n",
    "\n",
    "- Cross-Validation with a Holdout Set:\n",
    "\n",
    "In some cases, a holdout set (separate from the training and validation sets used in cross-validation) is kept aside for final model evaluation. The model is trained using cross-validation, and the best λ is chosen based on performance on the validation sets. The model's final evaluation is then performed on the holdout set.\n",
    "\n",
    "It's important to note that the effectiveness of different methods may vary depending on the specific characteristics of the dataset. **Cross-validation is a widely used and robust technique for hyperparameter tuning in Ridge Regression and other machine learning models.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bc1c1-072c-4484-826c-15434e839e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming X_train and y_train are your training data\n",
    "ridge = Ridge()\n",
    "param_grid = {'alpha': [0.1, 1, 10, 100]}  # alpha is equivalent to lambda\n",
    "\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_lambda = grid_search.best_params_['alpha']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b631cea-bebd-4fcd-a0a3-c95840aaefa6",
   "metadata": {},
   "source": [
    "## Q4. Can Ridge Regression be used for feature selection? If yes, how?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5929b202-e02e-4914-8715-1ac2adb89de3",
   "metadata": {},
   "source": [
    "#### Ridge Regression for Feature Selection:\n",
    "Ridge Regression does not perform variable selection in the same way as some other methods like LASSO (L1 regularization), which can lead coefficients to be exactly zero. However, it can shrink coefficients close to zero, effectively reducing the impact of less important features. It is not a feature selection technique in the strict sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fe4a71-24fa-4409-83a7-660ec6b71710",
   "metadata": {},
   "source": [
    "## Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e4088-7512-4e96-ae7f-a6393e820656",
   "metadata": {},
   "source": [
    "#### Handling Multicollinearity:\n",
    "\n",
    "Ridge Regression is particularly useful when multicollinearity is present. The regularization term added to the objective function helps to prevent the overfitting caused by high collinearity among predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2ce7ec-7cf1-47bc-81f8-8a44ea48ad05",
   "metadata": {},
   "source": [
    "## Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8719a2-57c5-47dd-85ef-755d4119d985",
   "metadata": {},
   "source": [
    "Ridge Regression is primarily designed for numerical (continuous) features, and its formulation assumes that the independent variables are quantitative. When dealing with a mix of categorical and continuous independent variables, additional preprocessing steps are typically needed to handle the categorical features appropriately.\n",
    "\n",
    "Here are some common approaches to incorporate both categorical and continuous variables when using Ridge Regression:\n",
    "\n",
    "- One-Hot Encoding for Categorical Variables:\n",
    "\n",
    "Convert categorical variables into binary (0/1) indicator variables using one-hot encoding. Each category is represented by a binary column, and the Ridge Regression model can then treat them as numerical features.\n",
    "One-hot encoding creates dummy variables, introducing additional columns equal to the number of categories minus one for each categorical variable.\n",
    "- Encoding Ordinal Variables:\n",
    "\n",
    "If the categorical variables are ordinal, meaning they have a meaningful order, you can encode them numerically based on their order. This preserves the ordinal information while allowing Ridge Regression to handle them as continuous variables.\n",
    "- Scaling Continuous Variables:\n",
    "\n",
    "Ridge Regression is sensitive to the scale of the input features. It's important to scale continuous variables before applying Ridge Regression to ensure that all features contribute equally to the regularization term.\n",
    "Common scaling methods include standardization (subtracting the mean and dividing by the standard deviation) or min-max scaling.\n",
    "- Regularization Parameter (λ):\n",
    "\n",
    "When using Ridge Regression, the regularization parameter (λ) controls the strength of the penalty term. The optimal value of λ should be determined through techniques like cross-validation, taking into account both the continuous and one-hot encoded categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c3fcae-a767-4c3e-b9d0-d7e4dd111c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Assuming X contains both continuous and categorical variables\n",
    "# Define which columns are continuous and categorical\n",
    "continuous_cols = [...]  # List of indices or column names for continuous variables\n",
    "categorical_cols = [...]  # List of indices or column names for categorical variables\n",
    "\n",
    "# Create a column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), continuous_cols),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a Ridge Regression model pipeline\n",
    "ridge_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "\n",
    "# Specify the hyperparameter grid for cross-validation\n",
    "param_grid = {\n",
    "    'ridge__alpha': [0.1, 1.0, 10.0]  # Adjust the alpha values as needed\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(ridge_pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Access the best Ridge Regression model\n",
    "best_ridge_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d896ab52-39ba-4239-abfa-fc96c5dd31ef",
   "metadata": {},
   "source": [
    "*In this example, ColumnTransformer is used to apply different preprocessing steps to the continuous and categorical columns. The Ridge Regression model is part of a pipeline that includes the preprocessing steps. The optimal value of the regularization parameter (alpha) is determined through cross-validation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65140703-bc43-41bc-970f-76ff00dc235d",
   "metadata": {},
   "source": [
    "## Q7. How do you interpret the coefficients of Ridge Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841c5eb-b544-478a-ae0f-3b60eb638431",
   "metadata": {},
   "source": [
    "Interpreting the coefficients in Ridge Regression is somewhat different from interpreting coefficients in ordinary least squares (OLS) regression due to the regularization term. In Ridge Regression, the coefficients are influenced not only by the relationship between the predictors and the target variable but also by the penalty applied to keep them from becoming too large. Here are some key points to consider when interpreting the coefficients in Ridge Regression:\n",
    "\n",
    "- Size and Significance:\n",
    "As in OLS regression, the sign of the Ridge Regression coefficients indicates the direction of the relationship between the predictor and the target variable.\n",
    "The magnitude of the coefficients should be considered in relation to the scale of the features. However, in Ridge Regression, the coefficients are shrunk towards zero, and their absolute values may be smaller than in OLS.\n",
    "\n",
    "- Relative Importance:\n",
    "The relative importance of predictors can be gauged by comparing the magnitudes of their coefficients. Larger absolute values suggest a stronger impact on the target variable.\n",
    "\n",
    "- Impact of Regularization:\n",
    "The regularization term in Ridge Regression tends to shrink coefficients towards zero. Therefore, the coefficients reflect both the original relationship between predictors and the target and the penalty applied to control the size of the coefficients.\n",
    "Even features with small coefficients may still be important in Ridge Regression, as long as the overall impact of the feature contributes to reducing the objective function (combining the goodness of fit and the regularization term).\n",
    "\n",
    "- Normalization Effect:\n",
    "The Ridge Regression coefficients are affected by the scale of the features. It's common practice to standardize or normalize the features before applying Ridge Regression to ensure that all features contribute equally to the regularization term.\n",
    "\n",
    "- No Exact Zero Coefficients:\n",
    "Unlike some other regularization techniques (e.g., LASSO), Ridge Regression rarely results in exact zero coefficients. Even features with small coefficients are retained in the model to some extent.\n",
    "\n",
    "- Interpretation of Dummy Variables (One-Hot Encoded):\n",
    "If categorical variables are one-hot encoded, the interpretation of coefficients involves comparing each category to the reference category. The coefficients indicate the change in the predicted target variable relative to the reference category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca10bba6-ec5b-4b61-9f64-6637514a9c72",
   "metadata": {},
   "source": [
    "## Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d301ff7-ddb9-4e7f-bd15-eb961201d802",
   "metadata": {},
   "source": [
    "Yes, Ridge Regression can be applied to time-series data analysis, but its use in this context is somewhat nuanced. Time-series data often has temporal dependencies, and traditional Ridge Regression assumes independence of observations. However, with some modifications and considerations, Ridge Regression can still be adapted for time-series analysis. Here are a few ways you can use Ridge Regression for time-series data:\n",
    "\n",
    "- Lagged Features:\n",
    "\n",
    "One common approach is to incorporate lagged values of the target variable or lagged features as predictors in the Ridge Regression model. This acknowledges the temporal dependencies in the data.\n",
    "For example, you might create lagged features such as the value of the target variable at the previous time step (lag 1), two time steps ago (lag 2), and so on.\n",
    "- Sliding Windows:\n",
    "\n",
    "Divide the time series into overlapping or non-overlapping windows and create features based on the statistics of each window. These statistics can include mean, standard deviation, and other relevant metrics.\n",
    "The Ridge Regression model can then be trained on these aggregated features.\n",
    "- Time-Based Features:\n",
    "\n",
    "Introduce time-based features that capture the temporal patterns in the data. This could include features like the day of the week, month, or quarter, which may have a cyclical influence on the target variable.\n",
    "Fourier transformations or other time-based representations might be used to extract periodic components.\n",
    "- Regularization Parameter Tuning:\n",
    "\n",
    "When applying Ridge Regression to time-series data, careful consideration should be given to tuning the regularization parameter (λ). This parameter controls the strength of the penalty term and should be selected based on cross-validation or other model selection techniques.\n",
    "The choice of λ impacts how much the model relies on the historical information contained in lagged features.\n",
    "- Stationarity:\n",
    "\n",
    "Ensure that the time series is stationary before applying Ridge Regression. Non-stationary time series may exhibit trends or seasonality that can affect the model's performance. Techniques such as differencing or detrending can be employed.\n",
    "- Performance Evaluation:\n",
    "\n",
    "Assess the model's performance using appropriate time-series evaluation metrics, such as mean absolute error (MAE), mean squared error (MSE), or others suitable for your specific application.\n",
    "Consider using time-series cross-validation techniques to obtain reliable estimates of the model's predictive performance.\n",
    "\n",
    "It's important to note that while Ridge Regression can be adapted for time-series data, there are other specialized techniques designed specifically for time-series forecasting, such as autoregressive integrated moving average (ARIMA), seasonal decomposition of time series (STL), and machine learning methods like recurrent neural networks (RNNs) or long short-term memory (LSTM) networks. The choice of method depends on the characteristics of the time-series data and the nature of the forecasting task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3563004f-2eee-49b0-b954-fdd314d3aed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming time-series data is organized with time dimension\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "ridge = Ridge()\n",
    "param_grid = {'alpha': [0.1, 1, 10, 100]}\n",
    "\n",
    "time_series_split = TimeSeriesSplit(n_splits=5)\n",
    "grid_search = GridSearchCV(ridge, param_grid, cv=time_series_split)\n",
    "grid_search.fit(X_time_series, y_time_series)\n",
    "\n",
    "best_lambda = grid_search.best_params_['alpha']\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
