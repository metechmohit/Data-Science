{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3740ef-31f9-45f7-9616-3f03740a46c1",
   "metadata": {},
   "source": [
    "## Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?"
   ]
  },
  {
   "attachments": {
    "d2fc675f-7db4-4d50-8a27-723d942ef5a3.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ae68bd56-3588-4ac0-bf93-97b30cf35e34",
   "metadata": {},
   "source": [
    "Polynomial functions and kernel functions are both used in machine learning algorithms, particularly in the context of support vector machines (SVMs) and kernel methods.\n",
    "\n",
    "1. **Polynomial Functions**: In the context of SVMs, polynomial functions are often used as kernel functions. These kernel functions compute the inner product between two feature vectors in a higher-dimensional space, allowing SVMs to learn non-linear decision boundaries in the original feature space. The polynomial kernel function is defined as ![image.png](attachment:d2fc675f-7db4-4d50-8a27-723d942ef5a3.png) are the input feature vectors, \\( d \\) is the degree of the polynomial, and \\( c \\) is a constant.\n",
    "\n",
    "\n",
    "2. **Kernel Functions**: Kernel functions are more general and can encompass various transformations of the input data to a higher-dimensional space. Besides polynomial kernels, other common kernel functions include Gaussian (or Radial Basis Function) kernels, sigmoid kernels, and more. These kernel functions allow SVMs to perform non-linear classification by implicitly mapping the input data into a higher-dimensional space where linear separation might be possible.\n",
    "\n",
    "In summary, polynomial functions can be seen as a specific type of kernel function used in SVMs, where they facilitate learning non-linear decision boundaries by implicitly mapping data to a higher-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b63897-36b2-4bc0-8c1d-aed1b12f74d0",
   "metadata": {},
   "source": [
    "## Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c9351c6-c06d-46c2-88f1-5263f186e9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate sample data\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Instantiate the SVM classifier with a polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3)\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = svm_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4887ed3-ecb6-49bb-af57-b328c151b5b9",
   "metadata": {},
   "source": [
    "## Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2461a4ca-da77-420a-b1f9-640426fae4df",
   "metadata": {},
   "source": [
    "Increasing the value of epsilon in Support Vector Regression (SVR) leads to a **decrease** in the number of support vectors. Here's why:\n",
    "\n",
    "* **Epsilon (ε):**  This parameter defines a tolerance zone around the hyperplane (decision function) in SVR. It essentially allows for a certain level of deviation from the perfect fit without incurring a penalty.\n",
    "\n",
    "* **Support Vectors:** These are the data points that lie closest to the hyperplane within the tolerance zone (ε-tube) or on its margins. They significantly influence the position and shape of the hyperplane.\n",
    "\n",
    "**Impact of Increasing Epsilon:**\n",
    "\n",
    "* **Wider Tolerance Zone:**  A larger epsilon widens the tolerance zone around the hyperplane. This allows more data points to fall within the zone without being considered for errors.\n",
    "\n",
    "* **Fewer Support Vectors:**  Since more points fall within the wider tolerance zone, fewer points will violate the margin and become support vectors. The model essentially becomes less strict about fitting the data points exactly, leading to a simpler model with fewer support vectors.\n",
    "\n",
    "**Trade-off:**\n",
    "\n",
    "* **Reduced Training Error:**  A larger epsilon can lead to a lower training error by allowing for more flexibility in fitting the data. \n",
    "\n",
    "* **Lower Accuracy:**  However, this comes at the cost of potentially lower accuracy on unseen data. A model with fewer support vectors might not capture the underlying relationships in the data as well, leading to less precise predictions for new data points.\n",
    "\n",
    "**Choosing Epsilon:**\n",
    "\n",
    "Selecting the optimal epsilon value involves finding a balance between reducing training error and maintaining good generalization (performance on unseen data). It's often achieved through cross-validation techniques where you experiment with different epsilon values and choose the one that yields the best performance on a validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846a9013-b7a4-4530-a990-314617ec52a7",
   "metadata": {},
   "source": [
    "## Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db356dae-32c5-4057-8294-ac31e939c0b4",
   "metadata": {},
   "source": [
    "###  Effect of Parameters on Support Vector Regression (SVR) Performance\n",
    "\n",
    "The performance of SVR is significantly influenced by the choice of kernel function and several key parameters:\n",
    "\n",
    "**1. Kernel Function:**\n",
    "\n",
    "* **Function:** The kernel function transforms the input data into a higher-dimensional feature space, allowing SVR to capture non-linear relationships between features that wouldn't be possible in the original space.\n",
    "\n",
    "* **Impact:** The choice of kernel function significantly affects the model's ability to learn complex patterns. Common choices include:\n",
    "    * **Linear Kernel:** Suitable for linear relationships but limited in capturing non-linearity.\n",
    "    * **Polynomial Kernel:** Can capture non-linearity but prone to overfitting with high degrees.\n",
    "    * **RBF Kernel:** A versatile kernel that works well for various data types and offers good balance between complexity and overfitting.\n",
    "\n",
    "* **Example:** If your data exhibits a clear non-linear pattern, using a polynomial or RBF kernel might be preferable to a linear kernel.\n",
    "\n",
    "**2. C Parameter (Regularization Parameter):**\n",
    "\n",
    "* **Function:** Controls the trade-off between model complexity and training error. A higher C allows for a more complex model that fits the training data closely but can lead to overfitting.\n",
    "\n",
    "* **Impact:**\n",
    "    * **Increase C:** Reduces training error but increases risk of overfitting, especially with noisy data.\n",
    "    * **Decrease C:** May lead to higher training error but can improve generalization (performance on unseen data).\n",
    "\n",
    "* **Example:** If your training data has a lot of noise, you might want to decrease C to prevent the model from overfitting to the noise.\n",
    "\n",
    "**3. Epsilon Parameter (ε):**\n",
    "\n",
    "* **Function:** Defines a tolerance zone around the hyperplane (decision function) in SVR. It allows for a certain level of deviation from the perfect fit without incurring a penalty.\n",
    "\n",
    "* **Impact:**\n",
    "    * **Increase Epsilon:** Reduces training error by allowing more flexibility in fitting the data (fewer support vectors). However, it can decrease model accuracy on unseen data.\n",
    "    * **Decrease Epsilon:**  Leads to a stricter model with lower training error but potentially lower accuracy if the model becomes too rigid.\n",
    "\n",
    "* **Example:** If your primary concern is obtaining a smooth fit with a lower training error and you're less worried about perfect accuracy on unseen data, you might increase epsilon. \n",
    "\n",
    "**4. Gamma Parameter (RBF Kernel Specific):**\n",
    "\n",
    "* **Function (For RBF Kernel Only):** Controls the influence of individual data points on the decision function. A higher gamma implies stronger influence from closer data points, leading to a more complex and potentially overfitting model.\n",
    "\n",
    "* **Impact (For RBF Kernel Only):**\n",
    "    * **Increase Gamma:** Leads to a more complex model that can capture intricate details but might overfit. \n",
    "    * **Decrease Gamma:** Creates a smoother, less complex model that might miss subtle patterns but can generalize better.\n",
    "\n",
    "* **Example:** If your data has a lot of local variations you want to capture, you might increase gamma. Conversely, for smoother, generalizable trends, decrease gamma.\n",
    "\n",
    "**Finding Optimal Values:**\n",
    "\n",
    "The optimal values for these parameters depend on your specific data and desired model behavior. Techniques like grid search or randomized search can be used to explore different parameter combinations and identify the configuration that yields the best performance on a validation set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4210db-3135-4062-a0ca-d499a8a2ea7a",
   "metadata": {},
   "source": [
    "## Q5. Assignment:\n",
    "- Import the necessary libraries and load the dataset\n",
    "- Split the dataset into training and testing sets\n",
    "- Preprocess the data using any technique of your choice (e.g. scaling, normalization)\n",
    "- Create an instance of the SVC classifier and train it on the training data\n",
    "- Use the trained classifier to predict the labels of the testing data\n",
    "- Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,recision, recall, F1-score)\n",
    "- Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performance\n",
    "- Train the tuned classifier on the entire dataset\n",
    "- Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9657094a-82a6-4228-a950-153db9d479ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "Best Parameters: {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['svm_classifier.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 4: Create an instance of the SVC classifier and train it on the training data\n",
    "svc = SVC()\n",
    "svc.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 5: Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test_scaled)\n",
    "\n",
    "# Step 6: Evaluate the performance of the classifier\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 7: Tune hyperparameters using GridSearchCV\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [0.1, 0.01, 0.001], 'kernel': ['linear', 'rbf', 'poly']}\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Step 8: Train the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X, y)\n",
    "\n",
    "# Step 9: Save the trained classifier to a file\n",
    "joblib.dump(best_svc, 'svm_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5adca1f-d2a5-4338-b4eb-fcf7ba744b59",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea8fb7dd-783f-4542-aba9-23066cab399f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db542775-7c08-4309-ad57-68c9b85a3fe4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9746389-d05d-45a2-b702-215b575edbc4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e581dbd-a7e5-4733-9874-1a0abd77e9a4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72b43591-42ca-4bd0-a7fe-8fa85cc2cf77",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a271a56-368a-46a4-958e-096102742519",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
